#!/usr/bin/env python3
"""
Vision Service using zQuery's proven LiteLLM factory
Provides spatial command analysis for Zeus_STT voice commands

Integrates with existing XPC bridge for seamless voice ‚Üí vision ‚Üí action pipeline
"""

import sys
import os
import base64
import json
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
import structlog

# Import zQuery's proven LiteLLM factory
sys.path.append('/Users/devin/dblitz/engine/src/gengines/zQuery/src/langchain')
from llm_factory import get_llm

# Set up logger
logger = structlog.get_logger()

class VisionContext:
    """Container for vision analysis results"""
    def __init__(self, 
                 target_text: Optional[str] = None,
                 spatial_relationship: Optional[str] = None,
                 confidence: float = 0.0,
                 bounds: Optional[Dict[str, int]] = None,
                 full_analysis: Optional[str] = None):
        self.target_text = target_text
        self.spatial_relationship = spatial_relationship  
        self.confidence = confidence
        self.bounds = bounds or {}
        self.full_analysis = full_analysis
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            "target_text": self.target_text,
            "spatial_relationship": self.spatial_relationship,
            "confidence": self.confidence,
            "bounds": self.bounds,
            "full_analysis": self.full_analysis
        }

class VisionService:
    """
    Vision service using GPT-4.1-mini for spatial command analysis
    
    Replaces failed local VLMs with proven cloud solution that achieves >95% accuracy
    """
    
    def __init__(self, disable_langfuse: bool = True):
        """Initialize vision service with zQuery's LiteLLM factory"""
        self.llm = None
        self.disable_langfuse = disable_langfuse
        
        # Disable Langfuse observability to avoid errors
        if disable_langfuse:
            os.environ["LANGFUSE_ENABLED"] = "false"
            # Also disable in LiteLLM
            import litellm
            litellm.success_callback = []
            litellm.failure_callback = []
            
        logger.info("üîç Initializing Vision Service with GPT-4.1-mini")
        
    def _get_llm(self):
        """Lazy load LLM to avoid initialization issues"""
        if self.llm is None:
            self.llm = get_llm("gpt-4.1-mini", max_tokens=32768, temperature=0.0)
            logger.info("‚úÖ GPT-4.1-mini vision model loaded successfully")
        return self.llm
    
    def _encode_image(self, image_path: str) -> str:
        """Encode image to base64 for vision API"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            logger.error("‚ùå Failed to encode image", error=str(e), path=image_path)
            raise
    
    def analyze_spatial_command(self, 
                              image_path: str, 
                              voice_command: str,
                              context: Optional[str] = None) -> VisionContext:
        """
        Analyze spatial voice command using vision
        
        Args:
            image_path: Path to screenshot
            voice_command: Voice command like "make this formal", "delete the text above"
            context: Optional context from memory
            
        Returns:
            VisionContext with spatial analysis results
        """
        try:
            logger.info("üîç Analyzing spatial command", 
                       command=voice_command, 
                       image=image_path)
            
            # Encode image
            base64_image = self._encode_image(image_path)
            
            # Construct analysis prompt
            prompt = self._build_spatial_prompt(voice_command, context)
            
            # Create vision messages
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ]
            
            # Get LLM response
            llm = self._get_llm()
            response = llm.complete(
                messages=messages,
                metadata={
                    "task": "spatial_command_analysis",
                    "command": voice_command,
                    "context": context or "none"
                }
            )
            
            content = response.choices[0].message.content
            
            # Parse spatial analysis
            vision_context = self._parse_spatial_response(content, voice_command)
            
            logger.info("‚úÖ Spatial analysis complete", 
                       target=vision_context.target_text,
                       confidence=vision_context.confidence)
            
            return vision_context
            
        except Exception as e:
            logger.error("‚ùå Spatial command analysis failed", 
                        error=str(e), 
                        command=voice_command)
            
            # Return empty context on failure
            return VisionContext(
                full_analysis=f"Error: {str(e)}",
                confidence=0.0
            )
    
    def _build_spatial_prompt(self, voice_command: str, context: Optional[str]) -> str:
        """Build optimized prompt for spatial command analysis"""
        
        base_prompt = f"""
You are analyzing a screenshot to resolve spatial references in voice commands for text editing.

VOICE COMMAND: "{voice_command}"
CONTEXT: {context or "None provided"}

Your task is to identify:
1. SPATIAL REFERENCES: Words like "this", "that", "above", "below", "next to"
2. TARGET TEXT: What specific text the user is referring to
3. SPATIAL RELATIONSHIP: How the target relates to other elements
4. CONFIDENCE: How certain you are about the identification (0.0-1.0)

ANALYSIS FORMAT:
- Be specific about what text is being referenced
- Describe the exact location (coordinates if possible)
- Note any visual cues that help identify the target
- Consider cursor position, selection highlights, or focus indicators

Focus on text editing contexts like:
- Code in editors (VS Code, etc.)
- Documents and text files  
- Terminal output
- UI text elements

Provide a clear, actionable analysis for voice-to-text automation.
"""
        
        return base_prompt.strip()
    
    def _parse_spatial_response(self, response: str, original_command: str) -> VisionContext:
        """Parse GPT-4.1-mini response into structured VisionContext"""
        
        # Simple parsing - could be enhanced with structured output
        target_text = None
        spatial_relationship = None
        confidence = 0.8  # Default high confidence for GPT-4.1-mini
        
        # Extract key information from response
        response_lower = response.lower()
        
        # Look for confidence indicators
        if any(word in response_lower for word in ['clearly', 'evident', 'obvious']):
            confidence = 0.9
        elif any(word in response_lower for word in ['appears', 'seems', 'likely']):
            confidence = 0.7
        elif any(word in response_lower for word in ['unclear', 'difficult', 'cannot']):
            confidence = 0.3
            
        # Extract spatial relationships
        if 'above' in response_lower:
            spatial_relationship = 'above'
        elif 'below' in response_lower:
            spatial_relationship = 'below'
        elif 'next to' in response_lower or 'beside' in response_lower:
            spatial_relationship = 'beside'
        elif 'this' in original_command.lower():
            spatial_relationship = 'current_selection'
        elif 'that' in original_command.lower():
            spatial_relationship = 'referenced_element'
        
        # Try to extract target text (basic implementation)
        # Look for quoted text or code blocks in response
        import re
        quoted_text = re.findall(r'"([^"]*)"', response)
        code_blocks = re.findall(r'`([^`]*)`', response)
        
        if quoted_text:
            target_text = quoted_text[0]
        elif code_blocks:
            target_text = code_blocks[0]
        
        return VisionContext(
            target_text=target_text,
            spatial_relationship=spatial_relationship,
            confidence=confidence,
            full_analysis=response
        )
    
    async def analyze_spatial_command_async(self, 
                                          image_path: str, 
                                          voice_command: str,
                                          context: Optional[str] = None) -> VisionContext:
        """Async version of spatial command analysis"""
        # Run sync version in thread pool for now
        # Could be enhanced with async LiteLLM calls
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self.analyze_spatial_command, 
            image_path, 
            voice_command, 
            context
        )
    
    def detect_visual_references(self, command: str) -> bool:
        """
        Detect if a voice command contains visual/spatial references
        
        Args:
            command: Voice command text
            
        Returns:
            True if command needs vision analysis
        """
        visual_keywords = [
            'this', 'that', 'these', 'those',
            'above', 'below', 'next to', 'beside',
            'here', 'there', 'up', 'down',
            'left', 'right', 'top', 'bottom',
            'current', 'selected', 'highlighted'
        ]
        
        command_lower = command.lower()
        has_visual_ref = any(keyword in command_lower for keyword in visual_keywords)
        
        if has_visual_ref:
            logger.info("üîç Visual reference detected", command=command)
            
        return has_visual_ref


# Global vision service instance
vision_service = VisionService()


# Convenience functions for XPC integration
def analyze_spatial_command(image_path: str, 
                          voice_command: str, 
                          context: Optional[str] = None) -> Dict[str, Any]:
    """
    Analyze spatial command (sync wrapper for XPC)
    
    Returns:
        Dictionary with vision analysis results
    """
    try:
        result = vision_service.analyze_spatial_command(image_path, voice_command, context)
        return result.to_dict()
    except Exception as e:
        logger.error("‚ùå Spatial analysis failed", error=str(e))
        return {
            "target_text": None,
            "spatial_relationship": None,
            "confidence": 0.0,
            "bounds": {},
            "full_analysis": f"Error: {str(e)}"
        }


def detect_visual_references(command: str) -> bool:
    """Check if command needs vision analysis (for XPC)"""
    return vision_service.detect_visual_references(command)


# Test function
def test_vision_service():
    """Test vision service with our VS Code screenshot"""
    print("üß™ Testing Vision Service Integration")
    print("=" * 50)
    
    image_path = "/Users/devin/Desktop/vision_test_768.png"
    
    test_commands = [
        "make this formal",
        "delete the text above", 
        "format this paragraph",
        "fix the code here"
    ]
    
    for command in test_commands:
        print(f"\nüîç Testing: '{command}'")
        
        # Check if command needs vision
        needs_vision = detect_visual_references(command)
        print(f"   Needs vision: {needs_vision}")
        
        if needs_vision:
            # Analyze with vision
            result = analyze_spatial_command(image_path, command)
            print(f"   Target: {result.get('target_text', 'None')}")
            print(f"   Relationship: {result.get('spatial_relationship', 'None')}")
            print(f"   Confidence: {result.get('confidence', 0.0):.2f}")


if __name__ == "__main__":
    test_vision_service()